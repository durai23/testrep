{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bottlenecks\n",
    "\n",
    "> High-performance computing is computing at the limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### On JURECA\n",
    "\n",
    "Each core runs at 2.5 GHz\n",
    "- 256 bit wide vector unit (4 double precision numbers)\n",
    "- 2 Fused multiply-add operations per cycle\n",
    "- Peak performance is 2.5 * 4 *2 * 2 GFlop/s = 40 GFlop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 12 cores per socket\n",
    "- Peak performance of a socket is 12 * 40 GFlop/s = 480 GFlop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are two sockets per node\n",
    "- Peak performance of a node (without GPUs) 960 GFlop/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the limit most people think of first, but it's often not the crucial one. Each core on JURECA can perform 40 GFlop/s if the code is completely *vectorized* and performs a *multiply and an add operation* at *each step*. If your code doesn't fulfill those requirements its peak performance will be less.\n",
    "\n",
    "Actually the peak performance is a little less because the AVX vector units run at a slightly lower frequency than the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The memory bandwidth of JURECA is about 60 GB/s per socket (12 cores). Let's look at a simple operation:\n",
    "\n",
    "c = c + a * b\n",
    "\n",
    "I assume that we are dealing with double precision numbers (8 bytes) then I have to read 3 * 8 bytes = 24 bytes and write 8 bytes. This is a multiply add operation, so each core can do 20 billion of those per second, but it only receives 60 GB/s / 24 bytes/op = 2.5Gop/s. This operation is clearly memory bound, if we have to get all the data from main memory.\n",
    "\n",
    "This is quite common. Let's look at a matrix multiplication $C=AB$. To calculate the element i, j of the result matrix C, we multiply row i of A with column j of B and sum the results. This is the scalar or dot product of row i of A and column j of B. In code this looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dot(a, b):\n",
    "    \"\"\"Multiply the matrix a with the matrix b.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: ndarray\n",
    "        left matrix\n",
    "    b: ndarray\n",
    "        right matrix\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    c: ndarray\n",
    "        result matrix\n",
    "    \"\"\"\n",
    "    c = np.zeros((a.shape[0], b.shape[1]))   \n",
    "    for i in range(a.shape[0]):             \n",
    "        for j in range(b.shape[1]):         \n",
    "            for k in range(a.shape[1]):     \n",
    "                 c[i, j] += a[i, k] * b[k, j]\n",
    "    return c                                \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take two small matrices A and B and see how long the above function takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 256\n",
    "a = np.random.random((n, n))\n",
    "b = np.random.random((n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 9.57 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A matrix multiplication of two n by n matrices performs $2n^3$ operations. The dot function achieves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "print(\"%.3f GFLOP/s\" % (2e-9 * n**3 / 9.57)) # Replace the last number with the time measured by %timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Wow, that's bad. Let's see if we can make this faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "jdot = jit(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 18.57 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 22.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit jdot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.465 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "print(\"%.3f GFLOP/s\" % (2e-9 * n**3 / 0.0229)) # Replace the last number with the time measured by %timeit converted to seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Access order and cache lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From our estimate above, we should be able to get at least twice this, but that's assuming we can achieve the maximum memory bandwidth. \n",
    "\n",
    "A numpy ndarray uses C-order (row-order) for storage. This means that the last index is continuous in memory and a change in any other index results in a jump from one memory location to another. The order of the loops therefore means that for both c and b, we don't get the maximum bandwidth, because we jump around and only use one element of the cache line. \n",
    "\n",
    "A datum is not loaded by itself. Instead everytime, a datum is needed that is not available in cache, a cache line containing the datum is loaded. On JURECA the cache line is 64 bytes wide. \n",
    "\n",
    "We can improve the performance by exchanging the loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def dot2(a, b):\n",
    "    \"\"\"Multiply the matrix a with the matrix b.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: ndarray\n",
    "        left matrix\n",
    "    b: ndarray\n",
    "        right matrix\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    c: ndarray\n",
    "        result matrix\n",
    "    \"\"\"\n",
    "    c = np.zeros((a.shape[0], b.shape[1]))   \n",
    "    for i in range(a.shape[0]):             \n",
    "        for k in range(a.shape[1]):     \n",
    "            for j in range(b.shape[1]):         \n",
    "                 c[i, j] += a[i, k] * b[k, j]\n",
    "    return c                                \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, elements in c and b are accessed in the proper order and a[i, k] is constant for the loop. This changes our estimate, because, now we read 16 bytes/op. This gives us a maximum of 60 GB/s / 16 bytes/op = 3.75 Gop/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 38.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 6.31 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dot2(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.317659587955626 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "print(2e-9 * n**3 / 0.00631, \"GFLOP/s\") # Replace the last number with the time measured by %timeit converted to seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is better than what we would expect from the bandwidth estimate, probably due to caching. One way to test this is to make the matrix larger, so that it doesn't fit in cache anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 4096\n",
    "a = np.random.random((n,n))\n",
    "b = np.random.random((n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 42 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dot2(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.088515808359551 GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "print(2e-9 * n**3 / 44.5, \"GFLOP/s\") # Replace the last number with the time measured by %timeit converted to seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is quite close to my estimate and shows that I have to increase the number of operations per byte loaded from main memory. Improving vectorization or using multiple threads wouldn't help.\n",
    "\n",
    "To improve cache utilization, we have to change the algorithm. One way to improve the performance of the matrix multiplication is blocking (aka tiling). This is done, e.g., in OpenBLAS or Intel's Math Kernel Library (MKL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's see how long numpy takes for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(2e-9 * n**3 / 1.53, \"GFLOP/s\") # Replace the last number with the time measured by %timeit converted to seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The numpy version we use here, uses a fast math library. That's what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### IBM GPFS\n",
    "$\\mathcal{O}(100)$ GB/s read/write speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each node connected to file system with 10 GBit/s or 1.25 GB/s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our GPFS achieves read/write bandwidths that are very similar to the main memory bandwidth, but not for a single node. Each node is connected to the GPFS with a 10 GBit/s connection. In other words, we can read/write about 1.25 GB/s. If we had to load the data in the previous calculation from disk, we could only achieve 1.25 GB/s / 24 bytes/op = 52 Mop/s. The main memory bandwidth or the peak performance of the CPU doesn't matter in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
