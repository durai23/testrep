{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI (Message Passing Interface) is the most used protocol for communicating between processes.  It doesn't matter if the processes that want to talk to each other are on the same or different nodes (i.e., computers). In this tutorial, we'll use `mpi4py` to learn about MPI and its API.\n",
    "\n",
    "An MPI program can run on a single computer/node with one or more processes that share memory or on multiple computers that are connected to each other via some network (not shown).\n",
    "\n",
    "![Six nodes](images/6nodes.png)\n",
    "\n",
    "\n",
    "\n",
    "## Communicators\n",
    "\n",
    "MPI processes talk to each other via communicators. The global communicator `MPI.COMM_WORLD` connects all MPI processes. Each process has a unique *rank* with `MPI.COMM_WORLD`. The process with `rank=0` is usually called the root process. MPI processes can participate in different communicators and may have a different rank in these communicators.\n",
    "\n",
    "![Communicators](images/6nodeswcommranks.png)\n",
    "\n",
    "## Point to point communication\n",
    "\n",
    "Sending messages from one MPI task to another is the foundation of MPI. Messages consist of some meta information such as the *source* and the *destination*, a *tag* that identifies the message, the *data type*, and the *count* of data items and the actual data.\n",
    "\n",
    "For each `Send` there has to be a matching `Recv`. This means that the meta information has to fit (including the tag). There are some wildcards and some additional commands that make this more flexible.\n",
    "\n",
    "Sending and receiving can be blocking or non-blocking. In the latter case, the flow of the program continuous after the call. In the former the program waits until the message has been transmitted. There is a very real danger for deadlocks here!\n",
    "\n",
    "![Point to point communication](images/6nodesptp.png)\n",
    "\n",
    "\n",
    "## Collective communication\n",
    "\n",
    "Collective calls are performed by all ranks of a communicator and thus *must* be called by all ranks. Examples for collective calls are `Bcast`, `Scatter`, `Gather`, and `Allreduce`. We'll look at some collective calls later in the notebook.\n",
    "\n",
    "![Collective Call](images/6nodescoll1.png)\n",
    "\n",
    "You could implement collective calls yourself using `Send` and `Recv`, but using the provided collective calls makes your code easier to read and allows vendors to optimize MPI for their platform. For example, in the picture above all the calls share the bandwidth of rank 0. One could use a tree-like pattern to balance the network load instead.\n",
    "\n",
    "![Collective Call Tree](images/6nodescoll2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about MPI at\n",
    "\n",
    "* http://materials.jeremybejarano.com/MPIwithPython/index.html\n",
    "\n",
    "And the documentation of mpi4py at http://mpi4py.scipy.org/docs/usrman/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the engines\n",
    "\n",
    "Before we can use MPI, we need to start some IPython engines. For this notebook, we'll use some local engines. The easiest way to start them is by typing ``ipcluster start --engines=MPIEngineSetLauncher`` into a terminal window. This will start as many engines as there are virtual processors on your machine.\n",
    "\n",
    "Please, go ahead and start the engines before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the engines\n",
    "\n",
    "Next, we want to connect to the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a view and activate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view = rc[:]\n",
    "view.activate()\n",
    "view.block = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for MPI\n",
    "\n",
    "We are now ready to use MPI with our IPython notebook.\n",
    "\n",
    "We will use the cell magic ``%%px`` and the line magic ``%px`` to execute commands on all the engines. So, whenenver you see ``%%px`` the cell is executed on *all* the engines, but not in the process that controls your notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px from mpi4py import MPI # Import and initialize MPI on the engines\n",
    "from mpi4py import MPI # Import and initialize MPI in the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing MPI initializes the MPI and sets up the default communicator `COMM_WORLD`, which includes all processes involved in this MPI program.\n",
    "\n",
    "Using `COMM_WORLD`, each process can determine its *rank* and the total number of ranks available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] I'm 2 of 8. Resistance is futile.\n",
      "[stdout:1] I'm 5 of 8. Resistance is futile.\n",
      "[stdout:2] I'm 6 of 8. Resistance is futile.\n",
      "[stdout:3] I'm 7 of 8. Resistance is futile.\n",
      "[stdout:4] I'm 4 of 8. Resistance is futile.\n",
      "[stdout:5] I'm 1 of 8. Resistance is futile.\n",
      "[stdout:6] I'm 3 of 8. Resistance is futile.\n",
      "[stdout:7] I'm 8 of 8. Resistance is futile.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"I'm %d of %d. Resistance is futile.\" % (rank + 1, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Point to point\n",
    "\n",
    "We can send messages from one rank to another. \n",
    "\n",
    "The following exercises will only work if more than one process is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] I'm rank one and I'm receiving a datum.\n",
      "[stdout:5] I'm rank zero and I'm sending a datum.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import sys\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "a = np.ones(1)\n",
    "b = np.zeros(1)\n",
    "if (size < 2):\n",
    "    print (\"Warning! Not enough ranks available!\" )\n",
    "else:\n",
    "    if rank == 0:\n",
    "        print (\"I'm rank zero and I'm sending a datum.\")\n",
    "        comm.Send(a[0], dest = 1) # Default destination is 0!\n",
    "    elif rank == 1:\n",
    "        print (\"I'm rank one and I'm receiving a datum.\")\n",
    "        comm.Recv(b, source = 0) # Default source is 0!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel reduction\n",
    "In the previous examples, we used send and receive. Now we are going to look at some collective communication. These commands involve all the ranks that belong to a communicator. Let's start by creating an array of random numbers and scattering it to all the ranks, i.e., each rank gets some part of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that are defined on the engines can be retrieved through the `view`. To get the variable rank, e.g., you can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks = view['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 5, 6, 3, 0, 2, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranks are not in any particular order, which can be annoying, but we can get the right order by casting ranks to an ndarray and calling argsort. The result gives us the engines in MPI rank order. Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranksort = np.array(ranks).argsort()\n",
    "rank0 = int(ranksort[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 6, 4, 1, 2, 3, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell is executed within the notebook. The engines don't know anything about it\n",
    "import numpy as np\n",
    "a = np.random.random(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First push the array to the engine with MPI rank 0\n",
    "%px a = None\n",
    "view.push({'a':a}, targets=rank0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we scatter the data from rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "a_partial = np.zeros(100000)\n",
    "comm.Scatter(a, a_partial, root = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the partial sum on each rank and then sum up the partial results using `Reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "total = np.zeros(1)\n",
    "sum_partial = np.sum(a_partial)\n",
    "comm.Reduce(sum_partial, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the view to get the result back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the random numbers is 50052.176449. The average is 0.500522.\n"
     ]
    }
   ],
   "source": [
    "total = view.pull('total', targets=rank0)\n",
    "print(\"The sum of the random numbers is %f. The average is %f.\" % (total, total / len(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper vs. lowercase in mpi4py\n",
    "`mpi4py` offers two version of many calls. The first one is written with in uppercase like the one in the previous cell. It uses memory buffers, e.g., `np.array`, and maps the call directly to the appropriate C call. The second version is written in lower case and takes arbitrary Python object. The result is given as the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] None\n",
      "[stdout:1] None\n",
      "[stdout:2] None\n",
      "[stdout:3] None\n",
      "[stdout:4] None\n",
      "[stdout:5] \n",
      "[array([ 0.55049318,  0.61523789,  0.3082229 , ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.29773394,  0.22057284,  0.02175943, ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.80176278,  0.27859166,  0.65503097, ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.24361521,  0.22960397,  0.99653137, ...,  0.        ,\n",
      "        0.        ,  0.        ]), array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.,  0.,  0., ...,  0.,  0.,  0.])]\n",
      "[stdout:6] None\n",
      "[stdout:7] None\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "a_all = comm.gather(a_partial)\n",
    "print(a_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `a_all` contains a `list` of `np.array`s.\n",
    "\n",
    "This second version is convenient, but it is **much** slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 100 loops, best of 3: 5.06 ms per loop\n",
      "[stdout:1] 100 loops, best of 3: 5.06 ms per loop\n",
      "[stdout:2] 100 loops, best of 3: 5.06 ms per loop\n",
      "[stdout:3] 100 loops, best of 3: 5.07 ms per loop\n",
      "[stdout:4] 100 loops, best of 3: 5.07 ms per loop\n",
      "[stdout:5] \n",
      "The slowest run took 4.33 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100 loops, best of 3: 5.05 ms per loop\n",
      "[stdout:6] 100 loops, best of 3: 5.07 ms per loop\n",
      "[stdout:7] 100 loops, best of 3: 5.06 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "%timeit a_all = comm.gather(a_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "The slowest run took 49.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:1] \n",
      "The slowest run took 172.82 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:2] \n",
      "The slowest run took 17.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:3] \n",
      "The slowest run took 33.13 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:4] \n",
      "The slowest run took 61.92 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:5] \n",
      "The slowest run took 126.58 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:6] \n",
      "The slowest run took 105.56 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "[stdout:7] \n",
      "The slowest run took 15.68 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 105 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "a_all = None\n",
    "if rank == 0:\n",
    "    a_all = np.zeros(len(a_partial))\n",
    "\n",
    "%timeit comm.Gather(a_partial[:len(a_partial) // size], a_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets worse as arrays get bigger.\n",
    "\n",
    "To retrieve the result, we use the `view` again and `ranksort[0]` gives us the index of rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25056.071341655923"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_all = np.array(view['a_all'])[ranksort[0]]\n",
    "a_all.sum() - a.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "While IPython notebooks are a nice way to teach about MPI and well tested MPI routines can be quite useful within a notebook, developing MPI code within notebooks can quickly become awkward because mistakes lead to blocking engines. I find it easier to write and test my MPI routines outside ipython notebooks and start the program with `mpiexec` from the command line.\n",
    "\n",
    "For example, you could write the following program and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helloparallelworld.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helloparallelworld.py\n",
    "from __future__ import print_function\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "numberOfRanks = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "a = np.zeros(1)\n",
    "if rank == 0:\n",
    "    print(\"There are %d MPI ranks.\" % numberOfRanks)\n",
    "    a = np.random.random(1)\n",
    "\n",
    "print(\"I'm rank %d.\" % rank)\n",
    "if rank == 0:\n",
    "    MPI.COMM_WORLD.Send(a, dest=1)\n",
    "elif rank == 1:\n",
    "    MPI.COMM_WORLD.Recv(a, source=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then switch to a terminal and execute it as `mpiexec -n 2 python helloparallelworld.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak to Peak\n",
    "Write a program that generates random numbers from a Gaussian distribution and then finds the minimum and maximum number generated.\n",
    "\n",
    "a) Generate the random numbers on rank 0 and scatter them. Calculate the maximum and minimum for the partial data for    \n",
    "   each rank and send the results back to rank 0. Find the maximum and minimum on rank 0 and compare it with numpy's\n",
    "   `ptp` function.\n",
    "   \n",
    "b) Generate random numbers at each rank. Calculate the minimum and maximum and use `Reduce` to find the extrema and send \n",
    "   them to rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gauss_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gauss_mpi.py\n",
    "from __future__ import print_function\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "numberOfRanks = comm.Get_size()\n",
    "\n",
    "N = 100000\n",
    "Nnode = int(N / numberOfRanks) + 1\n",
    "\n",
    "# compute numbers and scatter to workers\n",
    "if rank == 0:\n",
    "    print('\\nlocal min and max values')\n",
    "    sys.stdout.flush()\n",
    "    send_buffer = np.random.normal(size=(N))\n",
    "else:\n",
    "    send_buffer = np.zeros(Nnode)\n",
    "\n",
    "# initialize buffer for received data\n",
    "receive_buffer = np.zeros(Nnode)\n",
    "\n",
    "# scatter the data \n",
    "comm.Scatter(send_buffer, receive_buffer, root = 0)\n",
    "\n",
    "# compute minimal and maximal values\n",
    "gauss_min = np.array(np.min(receive_buffer))\n",
    "gauss_max = np.array(np.max(receive_buffer))\n",
    "\n",
    "print(rank, ':', gauss_min, gauss_max)\n",
    "\n",
    "\n",
    "# collect min and max values from all ranks\n",
    "all_gauss_min = np.zeros(numberOfRanks)\n",
    "all_gauss_max = np.zeros(numberOfRanks)\n",
    "\n",
    "comm.Gather(gauss_min, all_gauss_min)\n",
    "comm.Gather(gauss_max, all_gauss_max)\n",
    "\n",
    "if rank == 0:\n",
    "    print('\\nlocal min and max values collected')\n",
    "    print(all_gauss_min)\n",
    "    print(all_gauss_max)\n",
    "\n",
    "    \n",
    "# compute the global min and max values\n",
    "\n",
    "global_gauss_min = np.zeros(1)\n",
    "global_gauss_max = np.zeros(1)\n",
    "\n",
    "comm.Reduce(gauss_min, global_gauss_min, op=MPI.MIN, root=0)\n",
    "comm.Reduce(gauss_max, global_gauss_max, op=MPI.MAX, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('\\nglobal min and max values')\n",
    "    print('reduced:  ', global_gauss_min, global_gauss_max)\n",
    "    print('computed: ', np.min(send_buffer), np.max(send_buffer))\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
